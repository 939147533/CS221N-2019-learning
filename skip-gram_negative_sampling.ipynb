{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import collections\n",
    "import math\n",
    "import random\n",
    "import sys\n",
    "import time\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.utils.data as Data\n",
    "\n",
    "sys.path.append('..')\n",
    "import d2lzh_pytorch as d2l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_data_from_file(file_path):\n",
    "    with open(file_path, 'r') as fr:\n",
    "        lines = fr.readlines()\n",
    "    raw_dataset = [st.split() for st in lines]\n",
    "    print('sentence: %d' % len(raw_dataset))\n",
    "    return raw_dataset\n",
    "\n",
    "def load_data_from_text(text):\n",
    "    text = 'We are about to study the idea of a computational process.Computational processes are abstract beings that inhabit computers.As they evolve processes manipulate other abstract things called data'\n",
    "    sts = text.split('.')\n",
    "    raw_dataset = [st.split() for st in sts]\n",
    "    return raw_dataset\n",
    "\n",
    "    \n",
    "def token_index(raw_dataset):\n",
    "    counter = collections.Counter([tk for st in raw_dataset for tk in st])\n",
    "    counter = dict(filter(lambda x: x[1] >=1, counter.items()))\n",
    "    idx_to_token = [tk for tk,_ in counter.items()]\n",
    "    token_to_idx = {tk: idx for idx,tk in enumerate(idx_to_token)}\n",
    "    dataset = [[token_to_idx[tk] for tk in st if tk in token_to_idx] for st in raw_dataset]\n",
    "    num_tokens = sum([len(st) for st in dataset])\n",
    "    print('tokens: % d' % num_tokens)\n",
    "    return dataset, counter, idx_to_token, token_to_idx, num_tokens\n",
    "\n",
    "def discard(dataset, counter, num_tokens, idx_to_token, prop=1e-4):\n",
    "    '''\n",
    "    idx: 单词的数字索引\n",
    "    prop: 超参数，默认为1e-4\n",
    "    random.uniform(0,1)会随机产生0-1之间的数，当f_wi远大于prop，\n",
    "    即该单词出现次数很高，它与总词数之比远大于prop，\n",
    "    此时 1 - math.sqrt(prop / f_wi)近乎为1，\n",
    "    不等式成立，返回True,该词被丢弃，反之返回False,保留。\n",
    "    '''\n",
    "    subsampled_dataset = []\n",
    "    for st in dataset:\n",
    "        subsampled_st = []\n",
    "        for tk in st:\n",
    "            f_wi = counter[idx_to_token[tk]] / num_tokens\n",
    "            if not random.uniform(0,1) < 1 - math.sqrt(prop / f_wi):\n",
    "                subsampled_st.append(tk)\n",
    "        subsampled_dataset.append(subsampled_st)\n",
    "    sub_num_tokens = sum([len(st) for st in subsampled_dataset])\n",
    "    return subsampled_dataset, sub_num_tokens\n",
    "\n",
    "def get_centers_and_contexts(dataset, max_window_size):\n",
    "    centers, contexts = [], []\n",
    "    for st in dataset:\n",
    "        if len(st) < 2:\n",
    "            continue\n",
    "        centers += st\n",
    "        for center_i in range(len(st)):\n",
    "            window_size = random.randint(1, max_window_size)\n",
    "            indices = list(range(max(0, center_i - window_size), min(len(st), center_i + window_size + 1)))\n",
    "            indices.remove(center_i) # 去掉中心词\n",
    "            contexts.append([st[idx] for idx in indices])\n",
    "    return centers, contexts\n",
    "\n",
    "def get_negative(all_contexts, sampling_weights, K):\n",
    "    all_negatives, neg_candidates, i = [], [], 0\n",
    "    population = list(range(len(sampling_weights)))\n",
    "    for contexts in all_contexts:\n",
    "        negatives = []\n",
    "        while len(negatives) < len(contexts) * K:\n",
    "            if i == len(neg_candidates):\n",
    "                i = 0\n",
    "                # 从population中按照权重sampling_weights随机选取k(100000)个索引\n",
    "                neg_candidates = random.choices(population, sampling_weights, k=int(1e5))\n",
    "\n",
    "            neg = neg_candidates[i]\n",
    "            i += 1\n",
    "            if neg not in set(contexts): # 噪声词不能是背景词\n",
    "                negatives.append(neg)\n",
    "        all_negatives.append(negatives)\n",
    "    return all_negatives\n",
    "\n",
    "def load_all_data(file_path):\n",
    "    raw_dataset = load_data_from_file(file_path)\n",
    "    dataset, counter, idx_to_token, token_to_idx, num_tokens = token_index(raw_dataset)\n",
    "    subsampled_dataset, sub_num_tokens = discard(dataset, counter, num_tokens, idx_to_token, prop=1e-4)\n",
    "    all_centers, all_contexts = get_centers_and_contexts(subsampled_dataset, 5)\n",
    "    sampling_weights = [(counter[w] / sub_num_tokens)**0.75 for w in idx_to_token]\n",
    "    all_negatives = get_negative(all_contexts, sampling_weights, 5)\n",
    "    return all_centers, all_contexts, all_negatives, idx_to_token, token_to_idx\n",
    "\n",
    "def load_all_data2(text):\n",
    "    raw_dataset = load_data_from_text(text)\n",
    "    dataset, counter, idx_to_token, token_to_idx, num_tokens = token_index(raw_dataset)\n",
    "    all_centers, all_contexts = get_centers_and_contexts(dataset, 5)\n",
    "    weights = [(counter[w] / num_tokens)**0.75 for w in idx_to_token]\n",
    "    all_negatives = get_negative(all_contexts, dataset, 5)\n",
    "    return all_centers, all_contexts, all_negatives, idx_to_token, token_to_idx\n",
    "\n",
    "def skip_gram(center, contexts_and_negatives, embed_v, embed_u):\n",
    "    v = embed_v(center)\n",
    "    u = embed_u(contexts_and_negatives)\n",
    "    pred = torch.bmm(v, u.permute(0, 2, 1)) \n",
    "    return pred\n",
    "\n",
    "class MyDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, centers, contexts, negatives):\n",
    "        assert len(centers) == len(contexts) == len(negatives)\n",
    "        self.centers = centers\n",
    "        self.contexts = contexts\n",
    "        self.negatives = negatives\n",
    "    def __getitem__(self, index):\n",
    "        return (self.centers[index], self.contexts[index], self.negatives[index])\n",
    "    def __len__(self):\n",
    "        return len(self.centers)\n",
    "\n",
    "def batch_data(data):\n",
    "    \"\"\"\n",
    "    用作DataLoader的参数collate_fn\n",
    "    data: 长为batch_size的list，list中的每个元素都是Dataset类调用__getitem__得到结果\n",
    "    \"\"\"\n",
    "    max_len = max(len(c) + len(n) for _, c, n in data)\n",
    "    centers, contexts_negatives, masks, labels = [], [], [], []\n",
    "    for center, context, negative in data:\n",
    "        cur_len = len(context) + len(negative)\n",
    "        centers += [center]\n",
    "        contexts_negatives += [context + negative + [0] * (max_len - cur_len)]\n",
    "        masks += [[1]*cur_len + [0]*(max_len - cur_len)]\n",
    "        labels += [[1]*len(context) + [0]*(max_len - len(context))]\n",
    "    return (torch.tensor(centers).view(-1,1),\n",
    "           torch.tensor(contexts_negatives),\n",
    "           torch.tensor(masks), torch.tensor(labels))    \n",
    "\n",
    "class SigmoidBinaryCrossEntropyLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SigmoidBinaryCrossEntropyLoss, self).__init__()\n",
    "    def forward(self, inputs, targets, mask=None):\n",
    "        '''\n",
    "        input：predict， Tensor shape of (batch_size, len)\n",
    "        target：truth label，  Tensor of the same shape as input\n",
    "        mask: 用于指定batch中参与损失函数计算的部分预测值和标签\n",
    "        当掩码为1时，相应位置的预测值和标签将参与损失函数的计算；当掩码为0时，不参与计算\n",
    "        '''\n",
    "        inputs, targets, mask = inputs.float(), targets.float(), mask.float()\n",
    "        \n",
    "        res = nn.functional.binary_cross_entropy_with_logits(inputs, targets, reduction=\"none\", weight=mask)\n",
    "        return res.mean(dim=1) * mask.shape[1] /mask.float().sum(dim=1)\n",
    "    \n",
    "def train(net, data_iter, loss, optimizer, num_epochs):\n",
    "    \n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(\"train on\", device)\n",
    "    net = net.to(device)\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        start, l_sum, n = time.time(), 0.0, 0\n",
    "        for batch in data_iter:\n",
    "            center, context_negative, mask, label = [d.to(device) for d in batch]\n",
    "            pred = skip_gram(center, context_negative, net[0], net[1])\n",
    "            l = loss(pred.view(label.shape), label, mask).mean() # 一个batch的平均loss\n",
    "            optimizer.zero_grad() #梯度清零\n",
    "            l.backward() # 计算梯度\n",
    "            optimizer.step() # 权值更新\n",
    "            l_sum += l.cpu().item()\n",
    "            n += 1\n",
    "        print('epoch %d, loss %.2f, time %.2fs' % (epoch + 1, l_sum / n, time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentence: 42068\n",
      "tokens:  887521\n",
      "train on cuda\n",
      "epoch 1, loss 1.97, time 20.63s\n",
      "epoch 2, loss 0.62, time 18.31s\n",
      "epoch 3, loss 0.45, time 18.14s\n",
      "epoch 4, loss 0.39, time 18.14s\n",
      "epoch 5, loss 0.37, time 18.16s\n",
      "epoch 6, loss 0.35, time 18.19s\n",
      "epoch 7, loss 0.34, time 18.28s\n",
      "epoch 8, loss 0.33, time 18.15s\n",
      "epoch 9, loss 0.32, time 18.15s\n",
      "epoch 10, loss 0.32, time 18.19s\n",
      "epoch 11, loss 0.31, time 18.35s\n",
      "epoch 12, loss 0.31, time 18.41s\n",
      "epoch 13, loss 0.30, time 18.42s\n",
      "epoch 14, loss 0.30, time 19.38s\n",
      "epoch 15, loss 0.30, time 18.69s\n",
      "epoch 16, loss 0.29, time 18.36s\n",
      "epoch 17, loss 0.29, time 18.28s\n",
      "epoch 18, loss 0.29, time 19.35s\n",
      "epoch 19, loss 0.29, time 18.54s\n",
      "epoch 20, loss 0.28, time 18.60s\n",
      "epoch 21, loss 0.28, time 18.24s\n",
      "epoch 22, loss 0.28, time 18.37s\n",
      "epoch 23, loss 0.28, time 18.41s\n",
      "epoch 24, loss 0.28, time 18.30s\n",
      "epoch 25, loss 0.28, time 18.12s\n",
      "epoch 26, loss 0.28, time 18.19s\n",
      "epoch 27, loss 0.28, time 18.14s\n",
      "epoch 28, loss 0.28, time 18.16s\n",
      "epoch 29, loss 0.28, time 18.15s\n",
      "epoch 30, loss 0.27, time 18.10s\n",
      "epoch 31, loss 0.27, time 18.09s\n",
      "epoch 32, loss 0.27, time 18.13s\n",
      "epoch 33, loss 0.27, time 18.19s\n",
      "epoch 34, loss 0.27, time 18.23s\n",
      "epoch 35, loss 0.27, time 18.20s\n",
      "epoch 36, loss 0.27, time 18.37s\n",
      "epoch 37, loss 0.27, time 18.31s\n",
      "epoch 38, loss 0.27, time 18.45s\n",
      "epoch 39, loss 0.27, time 18.41s\n",
      "epoch 40, loss 0.27, time 18.24s\n",
      "epoch 41, loss 0.27, time 18.22s\n",
      "epoch 42, loss 0.27, time 18.28s\n",
      "epoch 43, loss 0.27, time 18.35s\n",
      "epoch 44, loss 0.27, time 18.39s\n",
      "epoch 45, loss 0.27, time 18.30s\n",
      "epoch 46, loss 0.27, time 18.37s\n",
      "epoch 47, loss 0.27, time 18.40s\n",
      "epoch 48, loss 0.27, time 18.23s\n",
      "epoch 49, loss 0.27, time 18.15s\n",
      "epoch 50, loss 0.27, time 18.37s\n",
      "epoch 51, loss 0.27, time 18.33s\n",
      "epoch 52, loss 0.27, time 18.33s\n",
      "epoch 53, loss 0.27, time 18.36s\n",
      "epoch 54, loss 0.27, time 18.31s\n",
      "epoch 55, loss 0.27, time 18.47s\n",
      "epoch 56, loss 0.27, time 18.44s\n",
      "epoch 57, loss 0.27, time 18.25s\n",
      "epoch 58, loss 0.26, time 18.35s\n",
      "epoch 59, loss 0.26, time 18.41s\n",
      "epoch 60, loss 0.26, time 18.31s\n",
      "epoch 61, loss 0.26, time 18.33s\n",
      "epoch 62, loss 0.26, time 18.35s\n",
      "epoch 63, loss 0.26, time 18.25s\n",
      "epoch 64, loss 0.26, time 18.27s\n",
      "epoch 65, loss 0.26, time 18.15s\n",
      "epoch 66, loss 0.26, time 18.19s\n",
      "epoch 67, loss 0.26, time 18.28s\n",
      "epoch 68, loss 0.26, time 18.23s\n",
      "epoch 69, loss 0.26, time 18.31s\n",
      "epoch 70, loss 0.26, time 18.28s\n",
      "epoch 71, loss 0.26, time 18.17s\n",
      "epoch 72, loss 0.26, time 18.40s\n",
      "epoch 73, loss 0.26, time 18.15s\n",
      "epoch 74, loss 0.26, time 18.16s\n",
      "epoch 75, loss 0.26, time 18.22s\n",
      "epoch 76, loss 0.26, time 18.16s\n",
      "epoch 77, loss 0.26, time 18.30s\n",
      "epoch 78, loss 0.26, time 18.19s\n",
      "epoch 79, loss 0.26, time 18.49s\n",
      "epoch 80, loss 0.26, time 18.29s\n",
      "epoch 81, loss 0.26, time 18.13s\n",
      "epoch 82, loss 0.26, time 18.12s\n",
      "epoch 83, loss 0.26, time 18.21s\n",
      "epoch 84, loss 0.26, time 18.48s\n",
      "epoch 85, loss 0.26, time 18.53s\n",
      "epoch 86, loss 0.26, time 18.43s\n",
      "epoch 87, loss 0.26, time 18.15s\n",
      "epoch 88, loss 0.26, time 18.13s\n",
      "epoch 89, loss 0.26, time 18.37s\n",
      "epoch 90, loss 0.26, time 18.31s\n",
      "epoch 91, loss 0.26, time 18.12s\n",
      "epoch 92, loss 0.26, time 18.22s\n",
      "epoch 93, loss 0.26, time 18.12s\n",
      "epoch 94, loss 0.26, time 18.10s\n",
      "epoch 95, loss 0.26, time 18.45s\n",
      "epoch 96, loss 0.26, time 18.35s\n",
      "epoch 97, loss 0.26, time 18.12s\n",
      "epoch 98, loss 0.26, time 18.47s\n",
      "epoch 99, loss 0.26, time 18.18s\n",
      "epoch 100, loss 0.26, time 18.36s\n"
     ]
    }
   ],
   "source": [
    "batch_size = 512\n",
    "num_workers = 0\n",
    "lr = 0.01\n",
    "num_epochs = 100\n",
    "# 读取数据\n",
    "file_path = r'E:\\ly\\Code\\Jupyter\\Pytorch\\Dive-into-DL-Pytorch\\Datasets\\ptb\\ptb.train.txt'\n",
    "all_centers, all_contexts, all_negatives, idx_to_token, token_to_idx = load_all_data(file_path)\n",
    "dataset = MyDataset(all_centers, all_contexts, all_negatives)\n",
    "data_iter = Data.DataLoader(dataset, batch_size, shuffle=True, collate_fn=batch_data, num_workers=num_workers)\n",
    "# 定义模型\n",
    "embed_size = 100\n",
    "net = nn.Sequential(\n",
    "        nn.Embedding(num_embeddings=len(token_to_idx), embedding_dim=embed_size),\n",
    "        nn.Embedding(num_embeddings=len(token_to_idx), embedding_dim=embed_size)\n",
    ")\n",
    "# 定义损失函数\n",
    "loss = SigmoidBinaryCrossEntropyLoss()\n",
    "# 定义优化函数\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=lr)\n",
    "# 进行训练\n",
    "train(net, data_iter, loss, optimizer, num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cosine sim=0.476: by\n",
      "cosine sim=0.451: u.s.\n",
      "cosine sim=0.437: in\n",
      "cosine sim=0.433: of\n",
      "cosine sim=0.427: massive\n",
      "cosine sim=0.419: political\n",
      "cosine sim=0.416: and\n",
      "cosine sim=0.408: up\n",
      "cosine sim=0.407: as\n",
      "cosine sim=0.406: to\n"
     ]
    }
   ],
   "source": [
    "def get_similar_tokens(query_token, k, embed):\n",
    "    W = embed.weight.data\n",
    "    x = W[token_to_idx[query_token]]\n",
    "    cos = torch.matmul(W, x) / (torch.sum(W * W, dim=1)\n",
    "                                * torch.sum(x * x) + 1e-9).sqrt()\n",
    "    _, topk = torch.topk(cos, k=k+1)\n",
    "    topk = topk.cpu().numpy()\n",
    "    for i in topk[1:]:\n",
    "        print('cosine sim=%.3f: %s' % (cos[i], (idx_to_token[i])))\n",
    "\n",
    "get_similar_tokens('government', 10, net[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cosine sim=0.471: mainframe\n",
      "cosine sim=0.468: nec\n",
      "cosine sim=0.441: equipped\n",
      "cosine sim=0.440: chips\n",
      "cosine sim=0.428: storage\n",
      "cosine sim=0.423: installed\n",
      "cosine sim=0.411: intel\n",
      "cosine sim=0.394: models\n",
      "cosine sim=0.392: siemens\n",
      "cosine sim=0.391: risc\n"
     ]
    }
   ],
   "source": [
    "get_similar_tokens('chip', 10, net[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
